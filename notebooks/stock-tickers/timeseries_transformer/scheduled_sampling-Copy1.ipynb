{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8190d02e-f5a6-4ad8-b998-34a80650067b",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e931d2-80fe-4728-8836-9215fa3dcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset Parameters\n",
    "dataset_daily_frequency = 24 # 24 corresponds to hourly data\n",
    "target_col_name = \"FCR_N_PriceEUR\"\n",
    "timestamp_col = \"timestamp\"\n",
    "cutoff_date = datetime.datetime(2017, 1, 1)\n",
    "\n",
    "# Scheduled Sampling Parameters\n",
    "x_mid = round(epochs * 0.55) # shift=0.5 converges f(midpoint) = 0.5; shifts scheduled_sampling_ratio curve further/closer from end of dataset\n",
    "start_scheduled_sampling_epoch = round(epochs * 0.2) # Adjust this value to start scheduled sampling after a certain epoch\n",
    "k = 0.3\n",
    "\n",
    "# Model Parameters\n",
    "dim_val = 512\n",
    "n_heads = 8\n",
    "n_decoder_layers = 4\n",
    "n_encoder_layers = 4\n",
    "dec_seq_len = 2 * dataset_daily_frequency\n",
    "enc_seq_len = 7 * dataset_daily_frequency # supposing you want the model to base its forecasts on the previous 5 days of data\n",
    "output_sequence_length = 2 * dataset_daily_frequency # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "step_size = 1\n",
    "batch_first = False\n",
    "num_predicted_features = 1\n",
    "\n",
    "# Hyperparameters\n",
    "test_size = 0.15\n",
    "validation_size = 0.15\n",
    "forecast_window = 2 * dataset_daily_frequency # supposing you're forecasting 48 hours ahead\n",
    "batch_size = 1 * forecast_window # 128\n",
    "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
    "\n",
    "# Define input variables\n",
    "exogenous_vars = []\n",
    "input_variables = [target_col_name] + exogenous_vars\n",
    "input_size = len(input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f440da-5510-4fb0-a6df-70e2af48adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scheduled sampling function (Sigmoid decay curve)\n",
    "def sigmoid_decay(x, x_mid, x_max, start_scheduled_sampling_epoch):\n",
    "    # Sigmoid function with scaling and shifting\n",
    "    return 1 / (1 + np.exp(-(x - start_scheduled_sampling_epoch - x_mid) / (k * (x_max - x_mid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de19dba4-64f2-4f63-a5ce-7d04f66053a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate x values\n",
    "x_values = np.linspace(0, epochs)\n",
    "\n",
    "# Calculate y values using the sigmoid decay function\n",
    "y_values = sigmoid_decay(x_values, x_mid, epochs, start_scheduled_sampling_epoch)\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(x_values, y_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Scheduled Sample Ratio')\n",
    "plt.title('Scheduled Sample Ratio')\n",
    "plt.grid(True)\n",
    "plt.axvline(x=start_scheduled_sampling_epoch, color='red', linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b89b6d-f8b8-4ac6-930c-905e82378e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file in data/dfs_merged_upload.csv\n",
      "From get_src_trg: data size = torch.Size([32189, 1])\n",
      "From get_src_trg: data size = torch.Size([6898, 1])\n",
      "From get_src_trg: data size = torch.Size([6898, 1])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import dataset as ds\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Read data\n",
    "data = utils.read_data('dfs_merged_upload')\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_split_idx = round(len(data) * (1 - test_size - validation_size))\n",
    "validation_split_idx = round(len(data) * (1 - test_size))\n",
    "training_data_raw = data[:train_split_idx]\n",
    "validation_data_raw = data[train_split_idx:validation_split_idx]\n",
    "test_data_raw = data[validation_split_idx:]\n",
    "\n",
    "# Get training indices\n",
    "training_indices = utils.get_indices_entire_sequence(\n",
    "    data=training_data_raw,\n",
    "    window_size=window_size,\n",
    "    step_size=step_size)\n",
    "\n",
    "# Get validation indices\n",
    "validation_indices = utils.get_indices_entire_sequence(\n",
    "    data=validation_data_raw,\n",
    "    window_size=window_size,\n",
    "    step_size=step_size)\n",
    "\n",
    "# Get test indices\n",
    "test_indices = utils.get_indices_entire_sequence(\n",
    "    data=test_data_raw,\n",
    "    window_size=window_size,\n",
    "    step_size=step_size)\n",
    "\n",
    "# Create custom dataset class for training data\n",
    "training_data = ds.TransformerDataset(\n",
    "    data=torch.tensor(training_data_raw[input_variables].values).float(),\n",
    "    indices=training_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length)\n",
    "\n",
    "# Create custom dataset class for validation data\n",
    "validation_data = ds.TransformerDataset(\n",
    "    data=torch.tensor(validation_data_raw[input_variables].values).float(),\n",
    "    indices=validation_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length)\n",
    "\n",
    "# Create custom dataset class for test data\n",
    "test_data = ds.TransformerDataset(\n",
    "    data=torch.tensor(test_data_raw[input_variables].values).float(),\n",
    "    indices=test_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "training_dataloader = DataLoader(training_data, batch_size)\n",
    "\n",
    "# Create DataLoader for validation data\n",
    "validation_dataloader = DataLoader(validation_data, batch_size)\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_dataloader = DataLoader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d726466b-7afa-451f-80f8-cf4ee50171c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_timeseries as tst\n",
    "\n",
    "# Initialize model\n",
    "model = tst.TimeSeriesTransformer(\n",
    "    input_size=input_size,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    batch_first=batch_first,\n",
    "    num_predicted_features=num_predicted_features,\n",
    "    dim_val=dim_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a633b348-07ca-49cf-b454-05eb28813a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_dataloader, optimizer, loss_function, scheduled_sampling_ratio):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "\n",
    "    for i, batch in enumerate(training_dataloader):\n",
    "        src, trg, trg_y = batch\n",
    "\n",
    "        # Scheduled sampling\n",
    "        if scheduled_sampling_ratio > np.random.rand():\n",
    "            trg = trg_y.unsqueeze(-num_predicted_features)  # Add a singleton dimension for the features\n",
    "\n",
    "        # Permute shape if needed\n",
    "        if not batch_first:\n",
    "            src = src.permute(1, 0, 2)\n",
    "            trg = trg.permute(1, 0, 2)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate masks\n",
    "        trg_mask = utils.generate_square_subsequent_mask(\n",
    "            dim1=forecast_window,\n",
    "            dim2=forecast_window,\n",
    "            device=device\n",
    "        )\n",
    "        src_mask = utils.generate_square_subsequent_mask(\n",
    "            dim1=forecast_window,\n",
    "            dim2=enc_seq_len,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Make forecasts\n",
    "        prediction = model(src, trg, src_mask, trg_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(trg_y, prediction)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append to training loss history\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Batch: {i}/{len(training_dataloader)}, Loss: {loss.item()} Scheduled Sampling Ratio: {scheduled_sampling_ratio}\")\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bf6f1f-d610-4d30-ad37-a558231f401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Put model in evaluation mode, calculate average loss across validation dataset.\n",
    "\"\"\"\n",
    "def validate_model(device, model, validation_dataloader, loss_function):\n",
    "        model.eval()\n",
    "        \n",
    "        loss_history = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (src, _, trg_y) in enumerate(validation_dataloader):\n",
    "\n",
    "                prediction = inference.run_encoder_decoder_inference(\n",
    "                    model=model, \n",
    "                    src=src, \n",
    "                    forecast_window=forecast_window,\n",
    "                    batch_size=src.shape[1],\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                trg_y = trg_y.permute(1, 0).unsqueeze(-1)  # Shape becomes [48, 256, 1]\n",
    "\n",
    "                print(f\"Shape of prediction: {prediction.shape}\")\n",
    "                print(f\"Shape of target: {trg_y.shape}\")\n",
    "                print(f\"Shape of src: {src.shape}\")\n",
    "\n",
    "\n",
    "                loss = loss_function(trg_y, prediction)\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1711328-576b-4c7c-afa1-4ada48284731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryneschroder/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([48, 48, 1])) that is different to the input size (torch.Size([48, 48])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Batch: 0/667, Loss: 827.7965698242188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 1/667, Loss: 360.9610900878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 2/667, Loss: 240.1166534423828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 3/667, Loss: 271.6027526855469 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 4/667, Loss: 210.41636657714844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 5/667, Loss: 167.38111877441406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 6/667, Loss: 163.34725952148438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 7/667, Loss: 101.90404510498047 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 8/667, Loss: 79.00782775878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 9/667, Loss: 89.78211212158203 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 10/667, Loss: 79.25994873046875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 11/667, Loss: 66.14630126953125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 12/667, Loss: 65.82469177246094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 13/667, Loss: 65.64776611328125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 14/667, Loss: 54.05138397216797 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 15/667, Loss: 46.857574462890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 16/667, Loss: 43.267822265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 17/667, Loss: 36.34220886230469 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 18/667, Loss: 35.937129974365234 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 19/667, Loss: 34.448429107666016 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 20/667, Loss: 59.420021057128906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 21/667, Loss: 70.82672119140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 22/667, Loss: 84.3136978149414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 23/667, Loss: 72.61736297607422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 24/667, Loss: 63.59347152709961 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 25/667, Loss: 63.45316696166992 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 26/667, Loss: 55.00593185424805 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 27/667, Loss: 64.05648040771484 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 28/667, Loss: 44.226444244384766 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 29/667, Loss: 44.24467468261719 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 30/667, Loss: 51.98176193237305 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 31/667, Loss: 76.97810363769531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 32/667, Loss: 129.69972229003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 33/667, Loss: 218.87509155273438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 34/667, Loss: 276.6546936035156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 35/667, Loss: 218.42628479003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 36/667, Loss: 172.01084899902344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 37/667, Loss: 136.00650024414062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 38/667, Loss: 138.85560607910156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 39/667, Loss: 113.49166107177734 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 40/667, Loss: 120.37908935546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 41/667, Loss: 173.11070251464844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 42/667, Loss: 135.16407775878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 43/667, Loss: 159.87193298339844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 44/667, Loss: 194.6937255859375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 45/667, Loss: 307.03399658203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 46/667, Loss: 311.7701721191406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 47/667, Loss: 225.4234619140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 48/667, Loss: 164.1910400390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 49/667, Loss: 177.4652862548828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 50/667, Loss: 189.7210235595703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 51/667, Loss: 332.6527099609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 52/667, Loss: 292.5219421386719 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 53/667, Loss: 195.05816650390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 54/667, Loss: 164.69581604003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 55/667, Loss: 348.4098815917969 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 56/667, Loss: 405.99359130859375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 57/667, Loss: 423.5894775390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 58/667, Loss: 502.0857238769531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 59/667, Loss: 442.21734619140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 60/667, Loss: 377.08282470703125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 61/667, Loss: 450.48919677734375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 62/667, Loss: 413.5423889160156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 63/667, Loss: 344.3408508300781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 64/667, Loss: 324.3014221191406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 65/667, Loss: 299.8623352050781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 66/667, Loss: 193.5876922607422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 67/667, Loss: 171.6549835205078 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 68/667, Loss: 163.8668670654297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 69/667, Loss: 155.8704833984375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 70/667, Loss: 248.31724548339844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 71/667, Loss: 289.46295166015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 72/667, Loss: 323.49114990234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 73/667, Loss: 309.7626647949219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 74/667, Loss: 439.68402099609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 75/667, Loss: 458.9073791503906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 76/667, Loss: 382.9394226074219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 77/667, Loss: 350.92779541015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 78/667, Loss: 402.8253479003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 79/667, Loss: 384.3940734863281 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 80/667, Loss: 285.908935546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 81/667, Loss: 251.9123077392578 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 82/667, Loss: 293.60284423828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 83/667, Loss: 314.6966857910156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 84/667, Loss: 269.5265808105469 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 85/667, Loss: 213.1161346435547 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 86/667, Loss: 220.3271026611328 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 87/667, Loss: 169.64846801757812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 88/667, Loss: 190.60174560546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 89/667, Loss: 212.23635864257812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 90/667, Loss: 220.50123596191406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 91/667, Loss: 208.7594757080078 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 92/667, Loss: 210.5950927734375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 93/667, Loss: 207.28225708007812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 94/667, Loss: 193.26229858398438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 95/667, Loss: 191.12925720214844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 96/667, Loss: 151.0917205810547 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 97/667, Loss: 134.78079223632812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 98/667, Loss: 151.64622497558594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 99/667, Loss: 156.37001037597656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 100/667, Loss: 180.08413696289062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 101/667, Loss: 161.51190185546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 102/667, Loss: 163.67242431640625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 103/667, Loss: 181.41229248046875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 104/667, Loss: 224.45852661132812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 105/667, Loss: 195.9072265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 106/667, Loss: 198.6105499267578 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 107/667, Loss: 217.57559204101562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 108/667, Loss: 207.18055725097656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 109/667, Loss: 193.37120056152344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 110/667, Loss: 204.93453979492188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 111/667, Loss: 191.50379943847656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 112/667, Loss: 129.61013793945312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 113/667, Loss: 123.41293334960938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 114/667, Loss: 147.57789611816406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 115/667, Loss: 124.06629180908203 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 116/667, Loss: 107.47689056396484 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 117/667, Loss: 97.67713928222656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 118/667, Loss: 101.01627349853516 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 119/667, Loss: 75.12644958496094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 120/667, Loss: 74.68862915039062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 121/667, Loss: 87.21263122558594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 122/667, Loss: 102.21240234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 123/667, Loss: 96.97572326660156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 124/667, Loss: 96.61841583251953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 125/667, Loss: 133.29429626464844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 126/667, Loss: 122.54623413085938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 127/667, Loss: 136.07174682617188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 128/667, Loss: 157.72410583496094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 129/667, Loss: 173.13780212402344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 130/667, Loss: 186.10128784179688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 131/667, Loss: 221.21963500976562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 132/667, Loss: 263.8561096191406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 133/667, Loss: 276.77642822265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 134/667, Loss: 263.7081604003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 135/667, Loss: 251.03623962402344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 136/667, Loss: 172.78445434570312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 137/667, Loss: 168.69488525390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 138/667, Loss: 166.33993530273438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 139/667, Loss: 155.6222686767578 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 140/667, Loss: 118.47215270996094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 141/667, Loss: 113.32291412353516 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 142/667, Loss: 94.40067291259766 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 143/667, Loss: 76.18677520751953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 144/667, Loss: 78.62763977050781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 145/667, Loss: 135.8321075439453 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 146/667, Loss: 349.802734375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 147/667, Loss: 118.52117156982422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 148/667, Loss: 104.46572875976562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 149/667, Loss: 106.78643798828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 150/667, Loss: 89.76648712158203 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 151/667, Loss: 60.98356246948242 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 152/667, Loss: 68.08523559570312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 153/667, Loss: 92.65911865234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 154/667, Loss: 60.747314453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 155/667, Loss: 49.114524841308594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 156/667, Loss: 61.40669631958008 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 157/667, Loss: 56.5389289855957 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 158/667, Loss: 52.48024368286133 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 159/667, Loss: 54.01827621459961 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 160/667, Loss: 46.359535217285156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 161/667, Loss: 36.57481002807617 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 162/667, Loss: 28.454591751098633 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 163/667, Loss: 29.23646354675293 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 164/667, Loss: 36.19215774536133 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 165/667, Loss: 31.370487213134766 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 166/667, Loss: 63.77787399291992 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 167/667, Loss: 59.516571044921875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 168/667, Loss: 27.03931999206543 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 169/667, Loss: 27.577112197875977 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 170/667, Loss: 24.0750675201416 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 171/667, Loss: 20.037031173706055 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 172/667, Loss: 16.78433609008789 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 173/667, Loss: 22.796667098999023 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 174/667, Loss: 728.1890258789062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 175/667, Loss: 178.46890258789062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 176/667, Loss: 63.58845901489258 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 177/667, Loss: 31.85027503967285 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 178/667, Loss: 38.356990814208984 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 179/667, Loss: 33.06327438354492 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 180/667, Loss: 31.79242706298828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 181/667, Loss: 37.295963287353516 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 182/667, Loss: 40.200355529785156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 183/667, Loss: 33.493038177490234 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 184/667, Loss: 31.93310546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 185/667, Loss: 42.68345260620117 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 186/667, Loss: 41.36772918701172 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 187/667, Loss: 31.235870361328125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 188/667, Loss: 32.36122131347656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 189/667, Loss: 28.865196228027344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 190/667, Loss: 38.808143615722656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 191/667, Loss: 31.550006866455078 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 192/667, Loss: 32.55315399169922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 193/667, Loss: 27.9932861328125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 194/667, Loss: 23.773740768432617 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 195/667, Loss: 24.532094955444336 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 196/667, Loss: 35.27912902832031 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 197/667, Loss: 36.13510513305664 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 198/667, Loss: 8.464863777160645 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 199/667, Loss: 8.919322967529297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 200/667, Loss: 10.167008399963379 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 201/667, Loss: 4.086684226989746 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 202/667, Loss: 3.171369791030884 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 203/667, Loss: 18.009485244750977 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 204/667, Loss: 41.36906814575195 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 205/667, Loss: 13.427714347839355 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 206/667, Loss: 6.112244606018066 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 207/667, Loss: 8.02995491027832 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 208/667, Loss: 8.454108238220215 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 209/667, Loss: 2.620830774307251 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 210/667, Loss: 3.2181520462036133 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 211/667, Loss: 1.7528632879257202 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 212/667, Loss: 0.8831836581230164 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 213/667, Loss: 1.7392325401306152 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 214/667, Loss: 1.9036728143692017 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 215/667, Loss: 5.35154914855957 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 216/667, Loss: 8.099163055419922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 217/667, Loss: 5.550065040588379 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 218/667, Loss: 6.73374080657959 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 219/667, Loss: 5.1715779304504395 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 220/667, Loss: 4.242753028869629 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 221/667, Loss: 3.4182281494140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 222/667, Loss: 4.814238548278809 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 223/667, Loss: 22.02442169189453 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 224/667, Loss: 13.895447731018066 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 225/667, Loss: 5.13139533996582 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 226/667, Loss: 10.317564010620117 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 227/667, Loss: 15.105132102966309 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 228/667, Loss: 292.4190979003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 229/667, Loss: 82.05745697021484 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 230/667, Loss: 134.67869567871094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 231/667, Loss: 133.10421752929688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 232/667, Loss: 144.444091796875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 233/667, Loss: 206.58145141601562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 234/667, Loss: 191.0677947998047 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 235/667, Loss: 166.7779998779297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 236/667, Loss: 142.1002197265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 237/667, Loss: 87.79327392578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 238/667, Loss: 117.39798736572266 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 239/667, Loss: 124.86223602294922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 240/667, Loss: 115.43827056884766 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 241/667, Loss: 120.97410583496094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 242/667, Loss: 363.5513000488281 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 243/667, Loss: 652.5032348632812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 244/667, Loss: 661.8826904296875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 245/667, Loss: 696.0338745117188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 246/667, Loss: 717.5069580078125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 247/667, Loss: 420.5005798339844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 248/667, Loss: 495.41400146484375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 249/667, Loss: 254.03355407714844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 250/667, Loss: 230.06524658203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 251/667, Loss: 131.93019104003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 252/667, Loss: 121.75066375732422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 253/667, Loss: 141.40040588378906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 254/667, Loss: 154.1705780029297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 255/667, Loss: 170.23355102539062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 256/667, Loss: 240.84747314453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 257/667, Loss: 282.6136779785156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 258/667, Loss: 230.5237274169922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 259/667, Loss: 220.5842742919922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 260/667, Loss: 216.70584106445312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 261/667, Loss: 175.4932403564453 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 262/667, Loss: 153.67015075683594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 263/667, Loss: 395.596435546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 264/667, Loss: 1266.40869140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 265/667, Loss: 918.8609619140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 266/667, Loss: 380.7721252441406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 267/667, Loss: 309.600830078125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 268/667, Loss: 745.2449340820312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 269/667, Loss: 563.9901123046875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 270/667, Loss: 426.3439025878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 271/667, Loss: 388.6927490234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 272/667, Loss: 607.1503295898438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 273/667, Loss: 925.5244750976562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 274/667, Loss: 424.1652526855469 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 275/667, Loss: 637.0160522460938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 276/667, Loss: 713.7303466796875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 277/667, Loss: 576.739501953125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 278/667, Loss: 655.1351318359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 279/667, Loss: 1503.29541015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 280/667, Loss: 1715.9415283203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 281/667, Loss: 1121.06396484375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 282/667, Loss: 2853.561767578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 283/667, Loss: 4313.81298828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 284/667, Loss: 3445.90625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 285/667, Loss: 2805.152587890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 286/667, Loss: 5798.09619140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 287/667, Loss: 3130.04931640625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 288/667, Loss: 2120.42529296875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 289/667, Loss: 7217.5966796875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 290/667, Loss: 6586.822265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 291/667, Loss: 2605.744140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 292/667, Loss: 892.69482421875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 293/667, Loss: 1077.387939453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 294/667, Loss: 1125.299072265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 295/667, Loss: 2079.366455078125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 296/667, Loss: 650.1050415039062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 297/667, Loss: 673.0792846679688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 298/667, Loss: 932.7753295898438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 299/667, Loss: 1016.5027465820312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 300/667, Loss: 971.088134765625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 301/667, Loss: 892.43994140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 302/667, Loss: 765.1837158203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 303/667, Loss: 552.3359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 304/667, Loss: 305.78155517578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 305/667, Loss: 223.160888671875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 306/667, Loss: 389.9734191894531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 307/667, Loss: 636.1268920898438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 308/667, Loss: 692.2894897460938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 309/667, Loss: 602.0314331054688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 310/667, Loss: 1411.8968505859375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 311/667, Loss: 1182.5977783203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 312/667, Loss: 1605.5848388671875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 313/667, Loss: 1552.999267578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 314/667, Loss: 943.8440551757812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 315/667, Loss: 685.2874145507812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 316/667, Loss: 636.7744750976562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 317/667, Loss: 445.67584228515625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 318/667, Loss: 351.81097412109375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 319/667, Loss: 467.0273742675781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 320/667, Loss: 331.46038818359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 321/667, Loss: 637.4330444335938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 322/667, Loss: 482.4214782714844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 323/667, Loss: 405.133544921875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 324/667, Loss: 339.28717041015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 325/667, Loss: 1343.7705078125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 326/667, Loss: 722.9390869140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 327/667, Loss: 327.08526611328125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 328/667, Loss: 286.5914001464844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 329/667, Loss: 356.18402099609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 330/667, Loss: 387.73089599609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 331/667, Loss: 275.2183532714844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 332/667, Loss: 237.60006713867188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 333/667, Loss: 185.46739196777344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 334/667, Loss: 165.35765075683594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 335/667, Loss: 238.14605712890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 336/667, Loss: 171.20494079589844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 337/667, Loss: 138.07386779785156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 338/667, Loss: 175.04351806640625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 339/667, Loss: 179.67391967773438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 340/667, Loss: 154.90548706054688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 341/667, Loss: 115.88955688476562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 342/667, Loss: 115.29039001464844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 343/667, Loss: 97.06731414794922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 344/667, Loss: 111.8846206665039 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 345/667, Loss: 132.1164093017578 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 346/667, Loss: 132.19122314453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 347/667, Loss: 113.48870849609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 348/667, Loss: 99.70429992675781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 349/667, Loss: 138.11740112304688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 350/667, Loss: 118.56381225585938 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 351/667, Loss: 83.95967102050781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 352/667, Loss: 81.46064758300781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 353/667, Loss: 96.55953216552734 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 354/667, Loss: 89.89359283447266 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 355/667, Loss: 94.35762023925781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 356/667, Loss: 81.88414764404297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 357/667, Loss: 154.3285675048828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 358/667, Loss: 172.82949829101562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 359/667, Loss: 112.88093566894531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 360/667, Loss: 196.43096923828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 361/667, Loss: 563.9757080078125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 362/667, Loss: 137.14395141601562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 363/667, Loss: 123.60166931152344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 364/667, Loss: 146.64474487304688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 365/667, Loss: 148.4852294921875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 366/667, Loss: 159.4185791015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 367/667, Loss: 230.7230987548828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 368/667, Loss: 172.00962829589844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 369/667, Loss: 189.80117797851562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 370/667, Loss: 179.17013549804688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 371/667, Loss: 129.33094787597656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 372/667, Loss: 124.0806655883789 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 373/667, Loss: 153.62892150878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 374/667, Loss: 192.57606506347656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 375/667, Loss: 141.5964813232422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 376/667, Loss: 140.95164489746094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 377/667, Loss: 162.7327423095703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 378/667, Loss: 128.9578094482422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 379/667, Loss: 109.35590362548828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 380/667, Loss: 155.8843536376953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 381/667, Loss: 171.64830017089844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 382/667, Loss: 131.1298370361328 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 383/667, Loss: 141.41212463378906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 384/667, Loss: 145.54844665527344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 385/667, Loss: 139.67544555664062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 386/667, Loss: 131.2374267578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 387/667, Loss: 122.11607360839844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 388/667, Loss: 114.87434387207031 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 389/667, Loss: 103.0978775024414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 390/667, Loss: 96.0499267578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 391/667, Loss: 108.66996765136719 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 392/667, Loss: 93.13751220703125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 393/667, Loss: 101.56105041503906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 394/667, Loss: 181.64736938476562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 395/667, Loss: 168.3509063720703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 396/667, Loss: 170.77700805664062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 397/667, Loss: 163.68141174316406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 398/667, Loss: 258.218505859375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 399/667, Loss: 279.67401123046875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 400/667, Loss: 279.89996337890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 401/667, Loss: 261.89825439453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 402/667, Loss: 208.42828369140625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 403/667, Loss: 202.957763671875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 404/667, Loss: 403.9689025878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 405/667, Loss: 511.6468811035156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 406/667, Loss: 380.30902099609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 407/667, Loss: 387.98162841796875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 408/667, Loss: 388.505859375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 409/667, Loss: 326.3657531738281 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 410/667, Loss: 310.9583740234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 411/667, Loss: 227.3841552734375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 412/667, Loss: 173.33306884765625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 413/667, Loss: 138.94357299804688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 414/667, Loss: 160.82989501953125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 415/667, Loss: 166.81134033203125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 416/667, Loss: 201.6793670654297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 417/667, Loss: 292.6839904785156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 418/667, Loss: 390.7439270019531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 419/667, Loss: 552.8141479492188 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 420/667, Loss: 839.3479614257812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 421/667, Loss: 1399.36474609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 422/667, Loss: 1180.9691162109375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 423/667, Loss: 692.0433349609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 424/667, Loss: 411.4291076660156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 425/667, Loss: 318.388916015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 426/667, Loss: 465.5482482910156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 427/667, Loss: 255.28570556640625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 428/667, Loss: 158.47364807128906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 429/667, Loss: 119.26880645751953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 430/667, Loss: 148.68067932128906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 431/667, Loss: 141.62840270996094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 432/667, Loss: 182.6363525390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 433/667, Loss: 334.56793212890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 434/667, Loss: 238.2421112060547 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 435/667, Loss: 215.79080200195312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 436/667, Loss: 146.05520629882812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 437/667, Loss: 159.27981567382812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 438/667, Loss: 188.37799072265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 439/667, Loss: 157.44241333007812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 440/667, Loss: 125.6634521484375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 441/667, Loss: 119.85994720458984 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 442/667, Loss: 163.8900146484375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 443/667, Loss: 151.74928283691406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 444/667, Loss: 124.51470184326172 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 445/667, Loss: 146.73858642578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 446/667, Loss: 138.97811889648438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 447/667, Loss: 129.27647399902344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 448/667, Loss: 161.22637939453125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 449/667, Loss: 168.32284545898438 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 450/667, Loss: 146.6481170654297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 451/667, Loss: 147.95578002929688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 452/667, Loss: 193.26290893554688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 453/667, Loss: 211.12408447265625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 454/667, Loss: 183.14715576171875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 455/667, Loss: 225.15420532226562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 456/667, Loss: 229.5374298095703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 457/667, Loss: 212.2071990966797 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 458/667, Loss: 167.13214111328125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 459/667, Loss: 167.90383911132812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 460/667, Loss: 192.9193878173828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 461/667, Loss: 122.61845397949219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 462/667, Loss: 115.0049819946289 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 463/667, Loss: 88.02049255371094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 464/667, Loss: 55.86830139160156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 465/667, Loss: 34.33159255981445 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 466/667, Loss: 27.241958618164062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 467/667, Loss: 18.649343490600586 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 468/667, Loss: 16.605419158935547 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 469/667, Loss: 18.56211280822754 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 470/667, Loss: 28.4892578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 471/667, Loss: 423.7341003417969 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 472/667, Loss: 510.464599609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 473/667, Loss: 189.39952087402344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 474/667, Loss: 256.0843811035156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 475/667, Loss: 474.1142272949219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 476/667, Loss: 198.40626525878906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 477/667, Loss: 226.95230102539062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 478/667, Loss: 394.39178466796875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 479/667, Loss: 345.3226318359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 480/667, Loss: 313.2217102050781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 481/667, Loss: 380.1883850097656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 482/667, Loss: 448.72308349609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 483/667, Loss: 279.2261657714844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 484/667, Loss: 286.60736083984375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 485/667, Loss: 287.7733459472656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 486/667, Loss: 258.2654113769531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 487/667, Loss: 269.7914123535156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 488/667, Loss: 250.02369689941406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 489/667, Loss: 375.55108642578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 490/667, Loss: 295.0726318359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 491/667, Loss: 253.68377685546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 492/667, Loss: 272.79534912109375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 493/667, Loss: 271.68695068359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 494/667, Loss: 335.26776123046875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 495/667, Loss: 332.2051086425781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 496/667, Loss: 279.43695068359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 497/667, Loss: 225.11997985839844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 498/667, Loss: 232.4358367919922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 499/667, Loss: 264.2893981933594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 500/667, Loss: 270.6380310058594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 501/667, Loss: 251.95143127441406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 502/667, Loss: 251.7066650390625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 503/667, Loss: 228.5130615234375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 504/667, Loss: 216.1811065673828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 505/667, Loss: 252.0623016357422 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 506/667, Loss: 253.59205627441406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 507/667, Loss: 230.49928283691406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 508/667, Loss: 239.3590087890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 509/667, Loss: 266.28173828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 510/667, Loss: 277.7223205566406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 511/667, Loss: 168.48770141601562 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 512/667, Loss: 162.6643524169922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 513/667, Loss: 152.24462890625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 514/667, Loss: 130.91427612304688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 515/667, Loss: 109.93793487548828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 516/667, Loss: 84.64903259277344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 517/667, Loss: 56.24280548095703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 518/667, Loss: 49.44362258911133 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 519/667, Loss: 46.83864212036133 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 520/667, Loss: 44.35625457763672 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 521/667, Loss: 40.58116912841797 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 522/667, Loss: 36.485748291015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 523/667, Loss: 38.25374221801758 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 524/667, Loss: 34.80336380004883 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 525/667, Loss: 32.3082389831543 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 526/667, Loss: 33.5330696105957 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 527/667, Loss: 47.625370025634766 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 528/667, Loss: 40.00260543823242 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 529/667, Loss: 35.229732513427734 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 530/667, Loss: 58.21639633178711 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 531/667, Loss: 98.01716613769531 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 532/667, Loss: 108.5004653930664 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 533/667, Loss: 70.43916320800781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 534/667, Loss: 65.74761199951172 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 535/667, Loss: 59.780052185058594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 536/667, Loss: 48.07016372680664 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 537/667, Loss: 52.31904983520508 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 538/667, Loss: 73.2916259765625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 539/667, Loss: 59.46656799316406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 540/667, Loss: 70.70387268066406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 541/667, Loss: 95.80445098876953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 542/667, Loss: 398.53759765625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 543/667, Loss: 629.8560791015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 544/667, Loss: 946.0257568359375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 545/667, Loss: 742.3655395507812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 546/667, Loss: 395.95440673828125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 547/667, Loss: 236.6552276611328 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 548/667, Loss: 178.69436645507812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 549/667, Loss: 169.81321716308594 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 550/667, Loss: 133.62452697753906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 551/667, Loss: 116.62903594970703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 552/667, Loss: 98.09101867675781 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 553/667, Loss: 115.71947479248047 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 554/667, Loss: 101.84651184082031 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 555/667, Loss: 101.2777099609375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 556/667, Loss: 105.75761413574219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 557/667, Loss: 110.15619659423828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 558/667, Loss: 128.0701446533203 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 559/667, Loss: 123.6149673461914 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 560/667, Loss: 113.34537506103516 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 561/667, Loss: 112.114013671875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 562/667, Loss: 103.45853424072266 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 563/667, Loss: 104.0067367553711 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 564/667, Loss: 105.79145050048828 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 565/667, Loss: 100.24969482421875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 566/667, Loss: 96.29045867919922 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 567/667, Loss: 66.36121368408203 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 568/667, Loss: 66.0189208984375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 569/667, Loss: 60.79315948486328 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 570/667, Loss: 48.91037368774414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 571/667, Loss: 40.57280349731445 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 572/667, Loss: 36.831268310546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 573/667, Loss: 29.428909301757812 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 574/667, Loss: 28.86546516418457 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 575/667, Loss: 29.837162017822266 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 576/667, Loss: 31.17571449279785 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 577/667, Loss: 26.5162296295166 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 578/667, Loss: 24.120285034179688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 579/667, Loss: 24.535812377929688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 580/667, Loss: 19.46858787536621 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 581/667, Loss: 18.05454444885254 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 582/667, Loss: 16.930503845214844 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 583/667, Loss: 16.676773071289062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 584/667, Loss: 14.885480880737305 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 585/667, Loss: 10.667717933654785 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 586/667, Loss: 7.695977210998535 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 587/667, Loss: 6.208516597747803 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 588/667, Loss: 4.818639755249023 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 589/667, Loss: 3.8473925590515137 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 590/667, Loss: 4.681303977966309 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 591/667, Loss: 6.847135066986084 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 592/667, Loss: 5.282301425933838 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 593/667, Loss: 4.983713150024414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 594/667, Loss: 4.143913745880127 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 595/667, Loss: 3.5402963161468506 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 596/667, Loss: 2.3070781230926514 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 597/667, Loss: 1.6487466096878052 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 598/667, Loss: 1.3341985940933228 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 599/667, Loss: 1.1190185546875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 600/667, Loss: 0.7795553207397461 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 601/667, Loss: 0.567237913608551 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 602/667, Loss: 0.8867897987365723 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 603/667, Loss: 2.893873453140259 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 604/667, Loss: 6.553040504455566 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 605/667, Loss: 7.16514778137207 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 606/667, Loss: 8.878069877624512 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 607/667, Loss: 8.43272590637207 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 608/667, Loss: 12.487645149230957 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 609/667, Loss: 16.759132385253906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 610/667, Loss: 26.351978302001953 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 611/667, Loss: 23.501646041870117 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 612/667, Loss: 30.223068237304688 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 613/667, Loss: 33.377601623535156 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 614/667, Loss: 51.916961669921875 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 615/667, Loss: 88.44225311279297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 616/667, Loss: 145.02615356445312 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 617/667, Loss: 191.2392578125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 618/667, Loss: 125.23551177978516 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 619/667, Loss: 118.7844009399414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 620/667, Loss: 121.60357666015625 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 621/667, Loss: 115.38831329345703 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 622/667, Loss: 182.5543670654297 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 623/667, Loss: 172.73878479003906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 624/667, Loss: 98.1781234741211 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 625/667, Loss: 58.80458068847656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 626/667, Loss: 37.747779846191406 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 627/667, Loss: 17.779600143432617 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 628/667, Loss: 19.964746475219727 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 629/667, Loss: 33.11515426635742 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 630/667, Loss: 17.931886672973633 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 631/667, Loss: 1.6316672563552856 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 632/667, Loss: 5.355623722076416 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 633/667, Loss: 5.725090980529785 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 634/667, Loss: 3.435431718826294 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 635/667, Loss: 6.116603374481201 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 636/667, Loss: 11.304819107055664 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 637/667, Loss: 6.512852668762207 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 638/667, Loss: 4.195483684539795 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 639/667, Loss: 6.331876277923584 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 640/667, Loss: 5.830306053161621 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 641/667, Loss: 6.549136161804199 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 642/667, Loss: 11.945362091064453 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 643/667, Loss: 11.493145942687988 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 644/667, Loss: 14.634740829467773 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 645/667, Loss: 13.397531509399414 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 646/667, Loss: 25.396089553833008 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 647/667, Loss: 50.85935592651367 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 648/667, Loss: 78.29942321777344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 649/667, Loss: 88.26515197753906 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 650/667, Loss: 96.83164978027344 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 651/667, Loss: 92.83715057373047 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 652/667, Loss: 94.77456665039062 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 653/667, Loss: 102.93064880371094 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 654/667, Loss: 104.33134460449219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 655/667, Loss: 109.41876220703125 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 656/667, Loss: 96.07793426513672 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 657/667, Loss: 77.833984375 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 658/667, Loss: 77.72196960449219 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 659/667, Loss: 52.492469787597656 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 660/667, Loss: 27.91411781311035 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 661/667, Loss: 8.43208122253418 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 662/667, Loss: 1.475985050201416 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 663/667, Loss: 1.3288747072219849 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 664/667, Loss: 2.034263849258423 Scheduled Sampling Ratio: 0.0\n",
      "Epoch: 0/10, Batch: 665/667, Loss: 2.0718562602996826 Scheduled Sampling Ratio: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryneschroder/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([48, 5, 1])) that is different to the input size (torch.Size([5, 48])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Batch: 666/667, Loss: 1.4145771265029907 Scheduled Sampling Ratio: 0.0\n",
      "Shape of prediction: torch.Size([48, 168, 1])\n",
      "Shape of target: torch.Size([48, 48, 1])\n",
      "Shape of src: torch.Size([48, 168, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryneschroder/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([48, 168, 1])) that is different to the input size (torch.Size([48, 48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (168) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m training_loss_history \u001b[38;5;241m=\u001b[39m train_model(model, training_dataloader, optimizer, loss_function, scheduled_sampling_ratio)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Validation phase  \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m validation_loss_history \u001b[38;5;241m=\u001b[39m validate_model(device, model, validation_dataloader, loss_function)\n\u001b[1;32m     45\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(training_loss_history)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(training_loss_history)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Average Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(validation_loss_history)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_loss_history)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Time (s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(device, model, validation_dataloader, loss_function)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrg_y\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of src: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_function(trg_y, prediction)\n\u001b[1;32m     28\u001b[0m         loss_history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_history\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (168) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demonstrating how to use the transformer model with time-series data.\n",
    "This code includes a full training loop with scheduled sampling and an evaluation step.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import inference\n",
    "from accelerate import Accelerator\n",
    "import time\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "training_loss_history = []\n",
    "validation_loss_history = []\n",
    "\n",
    "# Prepare device\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "model, optimizer, training_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    training_dataloader,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "training_loss_history = []\n",
    "validation_loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Scheduled Sampling\n",
    "    if epoch >= start_scheduled_sampling_epoch:\n",
    "        scheduled_sampling_ratio = sigmoid_decay(epoch, x_mid, epochs, start_scheduled_sampling_epoch)\n",
    "    else:\n",
    "        scheduled_sampling_ratio = 0.0  # Use teacher-forcing before start_scheduled_sampling_epoch\n",
    "    \n",
    "    # Training Phase    \n",
    "    training_loss_history = train_model(model, training_dataloader, optimizer, loss_function, scheduled_sampling_ratio)\n",
    "      \n",
    "    # Validation phase  \n",
    "    validation_loss_history = validate_model(device, model, validation_dataloader, loss_function)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, Average Training Loss: {sum(training_loss_history) / len(training_loss_history)} Average Validation Loss: {sum(validation_loss_history) / len(validation_loss_history)} Time (s): {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a12fe-28a6-4fa3-aba3-0370beae4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# view training\n",
    "training_loss_history = np.array(training_loss_history).reshape(-1)\n",
    "x = range(training_loss_history.shape[0])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, training_loss_history, label=\"train\")\n",
    "plt.title(\"Loss\", fontsize=15)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"nll\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846f985-879c-4b62-9970-2d2ce39936a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a583996-0674-418f-96b1-b2abb54302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "forecasts_ = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts_.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e4b0e-e411-49d6-9a7d-35b9250b40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda6807-9beb-4cc3-a3d6-564688aba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = np.vstack(forecasts_)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79061d95-b923-4de9-a1b2-8a129345cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1).squeeze(0).T\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "\n",
    "for item_id, ts in enumerate(test_data):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "        training=np.array(training_data),\n",
    "        periodicity=get_seasonality(freq),\n",
    "    )\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "\n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])\n",
    "    \n",
    "print(f\"MASE: {np.mean(mase_metrics)}\")\n",
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6291d69-f6b9-4247-8ee5-240536a6f0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
